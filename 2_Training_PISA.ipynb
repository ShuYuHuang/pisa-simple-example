{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c4ba126",
   "metadata": {},
   "source": [
    "# Training PISA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f826af",
   "metadata": {},
   "source": [
    "$^*$: 意指詳見reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea3a62b",
   "metadata": {},
   "source": [
    "先安裝mmdetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d5db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前面要先灌mmdetection\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install openmim\n",
    "!mim install mmdet==2.24.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcfd255",
   "metadata": {},
   "source": [
    "clone 我們的 github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ShuYuHuang/pisa-simple-example.git\n",
    "%cd pisa-simple-example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468bb36c",
   "metadata": {},
   "source": [
    "沒資料的話請下載資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b822f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/coco\n",
    "!curl -L \"https://public.roboflow.com/ds/teSUuBdtOy?key=MSfxYe5Fz3\" > data/coco/roboflow.zip\n",
    "!unzip -o data/coco/roboflow.zip -d data/coco\n",
    "!rm -rf data/coco/roboflow.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b3412",
   "metadata": {},
   "source": [
    "## 1.Configure file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bb155a",
   "metadata": {},
   "source": [
    "### 1.1.命名風格"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8f7f8",
   "metadata": {},
   "source": [
    "MMDetection對於Configure File的命名風格如下：\n",
    "{model}\\_{backbone}\\_{neck}\\_{schedule}\\_{dataset}\n",
    "\n",
    "- {model}： model 種類，例如：faster_rcnn, mask_rcnn 等。\n",
    "- {backbone}：Backbone 種類，例如：r50 (ResNet-50), x101 (ResNeXt-101) 等。\n",
    "- {neck}：Neck 種類，例如：fpn, pafpn, nasfpn, c4 等。\n",
    "- {schedule}： 訓練方案，選項是 1x、 2x、 20e 等。\n",
    "> 1x 和 2x 分別代表 12 epoch 和 24 epoch，20e 在級聯 (Cascaded) 模型中使用，表示 20 epoch。\n",
    "- {dataset}：資料集，例如：coco、 cityscapes、 voc_0712、 wider_face 等。\n",
    "\n",
    "例如pisa_faster_rcnn_x101_32x4d_fpn_1x_coco.py的意思就是:\n",
    "- 使用Prime sample attention(PISA)$^*$方案的FasterRCNN$^*$物件偵測模型 \n",
    "- Backbone用的是ResNext101$^*$，每個cell用的是32個group做concatenate，並且寬度為4\n",
    "- Neck用的是Feature Pyramid Networks(FPN)$^*$，在YOLOv4也有用\n",
    "- 訓練上這邊用的是 12 epoch\n",
    "- 訓練在coco dataset$^*$上"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89e9daf",
   "metadata": {},
   "source": [
    "### 1.2. Configure基本內容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ece52c",
   "metadata": {},
   "source": [
    "作者為了可以提高代碼複用率，所以 config 支持繼承的操作，通過 \\_base\\_ 變量來實現，\\_base\\_ 是一個 list 類型變量，裡面存放的是要繼承配置文件的路徑，任何配置文件都能往上追朔繼承以下四種類型的文件：\n",
    "- 模型 (models)\n",
    "- 資料集 (datasets)\n",
    "- 訓練策略 (schedules)\n",
    "- 運行時的默認配置 (default_runtime)\n",
    "\n",
    "在運算過程中mmdetection會把多個config檔整合成一個，所以後面我們下載下來的基本pisa config檔只有一個\n",
    "\n",
    "這邊看一下pisa_faster_rcnn_x101_32x4d_fpn_1x_coco.py 長怎樣："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "200cf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝mmdet時會順便安裝mmcv，是mm系列有關cv演算法的核心，其中包含config讀取\n",
    "from mmcv import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c13c5812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing pisa_faster_rcnn_x101_32x4d_fpn_1x_coco...\n",
      "\u001b[32mpisa_faster_rcnn_x101_32x4d_fpn_1x_coco-e4accec4.pth exists in /home/jovyan/course/pisa-simple-example/downloads\u001b[0m\n",
      "\u001b[32mSuccessfully dumped pisa_faster_rcnn_x101_32x4d_fpn_1x_coco.py to /home/jovyan/course/pisa-simple-example/downloads\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 沒有config檔的話請先下載\n",
    "!mkdir downloads\n",
    "!mim download mmdet --config pisa_faster_rcnn_x101_32x4d_fpn_1x_coco --dest ./downloads\n",
    "cfg=Config.fromfile(\"downloads/pisa_faster_rcnn_x101_32x4d_fpn_1x_coco.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc2e11b",
   "metadata": {},
   "source": [
    "大致上包含資料、模型、訓練、測試、推論的每個設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a839e2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'dataset_type', 'data_root', 'img_norm_cfg', 'train_pipeline', 'test_pipeline', 'data', 'evaluation', 'optimizer', 'optimizer_config', 'lr_config', 'runner', 'checkpoint_config', 'log_config', 'custom_hooks', 'dist_params', 'log_level', 'load_from', 'resume_from', 'workflow', 'opencv_num_threads', 'mp_start_method', 'auto_scale_lr'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c684e4b1",
   "metadata": {},
   "source": [
    "### 1.3.Model設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa003d3",
   "metadata": {},
   "source": [
    "Model裡面就把架構有系統的建構方式設定起來，也有有關訓練和測試的設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2be85e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['type', 'backbone', 'neck', 'rpn_head', 'roi_head', 'train_cfg', 'test_cfg'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2a991d",
   "metadata": {},
   "source": [
    "#### 1.3.1 類型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cffe63a",
   "metadata": {},
   "source": [
    "\n",
    "物件偵測模型類型，由於PISA演算法的核心是在roi_head的cnn blocks中實施self attention，所以與這個無關\n",
    "\n",
    "但是在論文上主要有使用到FasterRCNN來做主架構更動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d637a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FasterRCNN'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee5f93c",
   "metadata": {},
   "source": [
    "#### 1.3.2 Backbone模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41337e28",
   "metadata": {},
   "source": [
    "同個物件偵測模型中的backbone模型可以換置，會影響圖片的embedding效果，我們這邊用論文中做得較好的ResNext模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "215894c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'ResNeXt',\n",
       " 'depth': 101,\n",
       " 'num_stages': 4,\n",
       " 'out_indices': (0, 1, 2, 3),\n",
       " 'frozen_stages': 1,\n",
       " 'norm_cfg': {'type': 'BN', 'requires_grad': True},\n",
       " 'norm_eval': True,\n",
       " 'style': 'pytorch',\n",
       " 'init_cfg': {'type': 'Pretrained',\n",
       "  'checkpoint': 'open-mmlab://resnext101_32x4d'},\n",
       " 'groups': 32,\n",
       " 'base_width': 4}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d53bd66",
   "metadata": {},
   "source": [
    "#### 1.3.3 Neck模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4e5498",
   "metadata": {},
   "source": [
    "Neck 模型來將對應不同scale投射到不同解析度輸出時使用，使用FPN$^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b127bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'FPN',\n",
       " 'in_channels': [256, 512, 1024, 2048],\n",
       " 'out_channels': 256,\n",
       " 'num_outs': 5}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.neck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953975b0",
   "metadata": {},
   "source": [
    "#### 1.3.4 Region Proposal Network Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ae198",
   "metadata": {},
   "source": [
    "使用Region Proposal Network$^*$ 以前面Neck的資訊做物件區域偵測的提案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ec604f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'RPNHead',\n",
       " 'in_channels': 256,\n",
       " 'feat_channels': 256,\n",
       " 'anchor_generator': {'type': 'AnchorGenerator',\n",
       "  'scales': [8],\n",
       "  'ratios': [0.5, 1.0, 2.0],\n",
       "  'strides': [4, 8, 16, 32, 64]},\n",
       " 'bbox_coder': {'type': 'DeltaXYWHBBoxCoder',\n",
       "  'target_means': [0.0, 0.0, 0.0, 0.0],\n",
       "  'target_stds': [1.0, 1.0, 1.0, 1.0]},\n",
       " 'loss_cls': {'type': 'CrossEntropyLoss',\n",
       "  'use_sigmoid': True,\n",
       "  'loss_weight': 1.0},\n",
       " 'loss_bbox': {'type': 'L1Loss', 'loss_weight': 1.0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.rpn_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e142b9",
   "metadata": {},
   "source": [
    "#### 1.3.5 !!!Region of Interest Head!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1d34b",
   "metadata": {},
   "source": [
    "使用有計算proposal 對ground truth attention的Region of Interest$^*$ head 將最終預測算出。\n",
    "\n",
    "*這邊就是PISA將Attention加進model的地方*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712bbfe",
   "metadata": {},
   "source": [
    "roi_head會做一些基本的\n",
    "- bbox_roi_extractor: ROI pooling相關設定\n",
    "- bbox_head: Bounding box prediction head的設定，output head channel數、roi feature的大小、class數等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cb50dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'PISARoIHead',\n",
       " 'bbox_roi_extractor': {'type': 'SingleRoIExtractor',\n",
       "  'roi_layer': {'type': 'RoIAlign', 'output_size': 7, 'sampling_ratio': 0},\n",
       "  'out_channels': 256,\n",
       "  'featmap_strides': [4, 8, 16, 32]},\n",
       " 'bbox_head': {'type': 'Shared2FCBBoxHead',\n",
       "  'in_channels': 256,\n",
       "  'fc_out_channels': 1024,\n",
       "  'roi_feat_size': 7,\n",
       "  'num_classes': 80,\n",
       "  'bbox_coder': {'type': 'DeltaXYWHBBoxCoder',\n",
       "   'target_means': [0.0, 0.0, 0.0, 0.0],\n",
       "   'target_stds': [0.1, 0.1, 0.2, 0.2]},\n",
       "  'reg_class_agnostic': False,\n",
       "  'loss_cls': {'type': 'CrossEntropyLoss',\n",
       "   'use_sigmoid': False,\n",
       "   'loss_weight': 1.0},\n",
       "  'loss_bbox': {'type': 'SmoothL1Loss', 'loss_weight': 1.0, 'beta': 1.0}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.roi_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f506e",
   "metadata": {},
   "source": [
    "其他PISA實現演算法相關參數設定在cfg.model.train_cfg.rcnn中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb2b6a",
   "metadata": {},
   "source": [
    "##### Importance-based Sample Reweighting(ISR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a871b",
   "metadata": {},
   "source": [
    "都是要做Sample reweighting時做Hierarchical Local Rank(HLR)時用到的參數"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142573a7",
   "metadata": {},
   "source": [
    "在ISR中給negative proposal用Score做的HLR後計算attention式子的參數:\n",
    "- k: 調整attention分布尖銳程度用，對高HLR的偏袒程度\n",
    "- bias: 調整最低attention數值用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e14b1861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'ScoreHLRSampler',\n",
       " 'num': 512,\n",
       " 'pos_fraction': 0.25,\n",
       " 'neg_pos_ub': -1,\n",
       " 'add_gt_as_proposals': True,\n",
       " 'k': 0.5,\n",
       " 'bias': 0.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.train_cfg.rcnn.sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d5abd0",
   "metadata": {},
   "source": [
    "ISR中給positive proposal用IOU做的HLR計算attention式子的的參數:\n",
    "- k: 調整attention分布尖銳程度用，對高HLR的偏袒程度\n",
    "- bias: 調整最低attention數值用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58822466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 2, 'bias': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.train_cfg.rcnn.isr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656ee4b5",
   "metadata": {},
   "source": [
    "##### Classification-Aware Regression Loss(CARL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9167f290",
   "metadata": {},
   "source": [
    "使用class置信度計算做底計算Regression時的attention算式用到的參數:\n",
    "- k: 調整attention分布尖銳程度用，對高機率的偏袒程度\n",
    "- bias: 調整最低attention數值用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99690194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 1, 'bias': 0.2}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.train_cfg.rcnn.carl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eacc0d",
   "metadata": {},
   "source": [
    "### 1.4.資料設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095299be",
   "metadata": {},
   "source": [
    "- xxx_per_gpu: 一些GPU設定\n",
    "- data.train/val/test: 訓練、驗證、測試集都有不同的設定 (下面以train來講解)\n",
    "- data.train.type: 資料集類型，主要是標註的格式\n",
    "- data.train.ann_file: 標註檔位置\n",
    "- data.train.img_prefix: 圖檔資料夾位置\n",
    "- data.train.pipeline: 用list方式列出pre-process會用的演算法，可能有normalize大小、翻轉、旋轉等等可以用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ff4ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CocoDataset\n",
      "data/coco/annotations/instances_train2017.json\n",
      "data/coco/train2017/\n"
     ]
    }
   ],
   "source": [
    "print(cfg.data.train.type)\n",
    "print(cfg.data.train.ann_file)\n",
    "print(cfg.data.train.img_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87b92b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'LoadImageFromFile'}\n",
      "{'type': 'LoadAnnotations', 'with_bbox': True}\n",
      "{'type': 'Resize', 'img_scale': (1333, 800), 'keep_ratio': True}\n",
      "{'type': 'RandomFlip', 'flip_ratio': 0.5}\n",
      "{'type': 'Normalize', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}\n",
      "{'type': 'Pad', 'size_divisor': 32}\n",
      "{'type': 'DefaultFormatBundle'}\n",
      "{'type': 'Collect', 'keys': ['img', 'gt_bboxes', 'gt_labels']}\n"
     ]
    }
   ],
   "source": [
    "print(*cfg.data.train.pipeline,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e1b4fc",
   "metadata": {},
   "source": [
    "### 1.5. 更改cfg內容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb4735",
   "metadata": {},
   "source": [
    "藉由引用cfg檔我們可以繼承屬性，並只修改其中一部分，我們可以簡單做出一個符合資料集以及model設定的cfg檔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c9d74e",
   "metadata": {},
   "source": [
    "#### 1.5.1 自編cfg檔內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b72d1d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# The new config inherits a base config to highlight the necessary modification\n",
      "_base_ = ['downloads/pisa_faster_rcnn_x101_32x4d_fpn_1x_coco.py']\n",
      "\n",
      "# We also need to change the num_classes in head to match the dataset's annotation\n",
      "model = dict(\n",
      "    roi_head=dict(\n",
      "        bbox_head=dict(\n",
      "            num_classes=13)),\n",
      "    train_cfg=dict(\n",
      "        rpn_proposal=dict(\n",
      "            nms_pre=200,\n",
      "            max_per_img=200)),\n",
      "    test_cfg=dict(\n",
      "        rpn=dict(\n",
      "            nms_pre=200,\n",
      "            max_per_img=200))\n",
      ")\n",
      "# Modify dataset related settings\n",
      "dataset_type = 'COCODataset'\n",
      "classes = ('bishop',\n",
      "'black-bishop',\n",
      " 'black-king',\n",
      " 'black-knight',\n",
      " 'black-pawn',\n",
      " 'black-queen',\n",
      " 'black-rook',\n",
      " 'white-bishop',\n",
      " 'white-king',\n",
      " 'white-knight',\n",
      " 'white-pawn',\n",
      " 'white-queen',\n",
      " 'white-rook')\n",
      "data_root = 'data/coco/'\n",
      "\n",
      "data = dict(\n",
      "    train=dict(\n",
      "        img_prefix='data/coco/train/',\n",
      "        classes=classes,\n",
      "        ann_file='data/coco/train/_annotations.coco.json'),\n",
      "    val=dict(\n",
      "        img_prefix='data/coco/valid/',\n",
      "        classes=classes,\n",
      "        ann_file='data/coco/valid/_annotations.coco.json'),\n",
      "    test=dict(\n",
      "        img_prefix='data/coco/test/',\n",
      "        classes=classes,\n",
      "        ann_file='data/coco/test/_annotations.coco.json',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]))"
     ]
    }
   ],
   "source": [
    "# 想要看元檔可以跑這行\n",
    "!cat pisa_chess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3dc717",
   "metadata": {},
   "source": [
    "**繼承**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e4976",
   "metadata": {},
   "source": [
    "先繼承一個已經很完整的設定檔，其中已經有關於model, data, training, device相關設定\n",
    "```python\n",
    "_base_ = ['downloads/pisa_faster_rcnn_x101_32x4d_fpn_1x_coco.py']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d9642",
   "metadata": {},
   "source": [
    "**修改模型設定**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade4d0fe",
   "metadata": {},
   "source": [
    "``` python\n",
    "model = dict(\n",
    "    roi_head=dict(\n",
    "        bbox_head=dict(\n",
    "            num_classes=13)),\n",
    "    train_cfg=dict(\n",
    "        rpn_proposal=dict(\n",
    "            nms_pre=200,\n",
    "            max_per_img=200)),\n",
    "    test_cfg=dict(\n",
    "        rpn=dict(\n",
    "            nms_pre=200,\n",
    "            max_per_img=200))\n",
    ")\n",
    "```\n",
    "- roi_head的class數量:這最重要，因為基礎設定是coco的80類別，這邊就看我們的任務要幾類\n",
    "- train_cfg.rpn_proposal: non-maximum suppression的數量設定，如果比coco簡單或許可以設小一點加快訓練\n",
    "- test_cfg.rpn: non-maximum suppression的數量設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec866df",
   "metadata": {},
   "source": [
    "**類別與母資料夾名稱**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec60b46",
   "metadata": {},
   "source": [
    "這邊要將classes改成自己對應的label(照順序)，並且給定data存放的母資料夾\n",
    "```python\n",
    "classes = ('bishop',\n",
    "'black-bishop',\n",
    " 'black-king',\n",
    " 'black-knight',\n",
    " 'black-pawn',\n",
    " 'black-queen',\n",
    " 'black-rook',\n",
    " 'white-bishop',\n",
    " 'white-king',\n",
    " 'white-knight',\n",
    " 'white-pawn',\n",
    " 'white-queen',\n",
    " 'white-rook')\n",
    "data_root = 'data/coco/'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd21ba6d",
   "metadata": {},
   "source": [
    "**修改資料讀取設定**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154a356b",
   "metadata": {},
   "source": [
    "指定資料和標註給訓練、驗證、測試使用\n",
    "```python\n",
    "data = dict(\n",
    "    train=dict(\n",
    "        img_prefix='data/coco/train/',\n",
    "        classes=classes,\n",
    "        ann_file='data/coco/train/_annotations.coco.json'),\n",
    "    val=dict(\n",
    "        img_prefix='data/coco/valid/',\n",
    "        classes=classes,\n",
    "        ann_file='data/coco/valid/_annotations.coco.json'),\n",
    "    test=dict(\n",
    "        img_prefix='data/coco/test/',\n",
    "        classes=classes,\n",
    "        ann_file='data/coco/test/_annotations.coco.json'))\n",
    "```\n",
    "- img_prefix: 影像資料夾名稱\n",
    "- classes: 用剛剛指定的class名稱\n",
    "- ann_file: 標註檔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38535a76",
   "metadata": {},
   "source": [
    "想要修改augmentation可以修改pipeline，例如:\n",
    "```python\n",
    "    test=dict(\n",
    "        img_prefix='data/coco/test/',\n",
    "        classes=classes,\n",
    "        ann_file='data/coco/test/_annotations.coco.json',\n",
    "        pipeline=[\n",
    "            dict(type='LoadImageFromFile'),\n",
    "            dict(\n",
    "                type='MultiScaleFlipAug',\n",
    "                img_scale=(1333, 800),\n",
    "                flip=False,\n",
    "                transforms=[\n",
    "                    dict(type='Resize', keep_ratio=True),\n",
    "                    dict(type='RandomFlip'),\n",
    "                    dict(\n",
    "                        type='Normalize',\n",
    "                        mean=[123.675, 116.28, 103.53],\n",
    "                        std=[58.395, 57.12, 57.375],\n",
    "                        to_rgb=True),\n",
    "                    dict(type='Pad', size_divisor=32),\n",
    "                    dict(type='ImageToTensor', keys=['img']),\n",
    "                    dict(type='Collect', keys=['img'])\n",
    "                ])\n",
    "        ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54078433",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dd2da3",
   "metadata": {},
   "source": [
    "有了設定檔就可以直接開始train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9d4c8b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] [--work-dir WORK_DIR] [--resume-from RESUME_FROM]\n",
      "                [--auto-resume] [--no-validate]\n",
      "                [--gpus GPUS | --gpu-ids GPU_IDS [GPU_IDS ...] | --gpu-id\n",
      "                GPU_ID] [--seed SEED] [--diff-seed] [--deterministic]\n",
      "                [--options OPTIONS [OPTIONS ...]]\n",
      "                [--cfg-options CFG_OPTIONS [CFG_OPTIONS ...]]\n",
      "                [--launcher {none,pytorch,slurm,mpi}]\n",
      "                [--local_rank LOCAL_RANK] [--auto-scale-lr]\n",
      "                config\n",
      "\n",
      "Train a detector\n",
      "\n",
      "positional arguments:\n",
      "  config                train config file path\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --work-dir WORK_DIR   the dir to save logs and models\n",
      "  --resume-from RESUME_FROM\n",
      "                        the checkpoint file to resume from\n",
      "  --auto-resume         resume from the latest checkpoint automatically\n",
      "  --no-validate         whether not to evaluate the checkpoint during training\n",
      "  --gpus GPUS           (Deprecated, please use --gpu-id) number of gpus to\n",
      "                        use (only applicable to non-distributed training)\n",
      "  --gpu-ids GPU_IDS [GPU_IDS ...]\n",
      "                        (Deprecated, please use --gpu-id) ids of gpus to use\n",
      "                        (only applicable to non-distributed training)\n",
      "  --gpu-id GPU_ID       id of gpu to use (only applicable to non-distributed\n",
      "                        training)\n",
      "  --seed SEED           random seed\n",
      "  --diff-seed           Whether or not set different seeds for different ranks\n",
      "  --deterministic       whether to set deterministic options for CUDNN\n",
      "                        backend.\n",
      "  --options OPTIONS [OPTIONS ...]\n",
      "                        override some settings in the used config, the key-\n",
      "                        value pair in xxx=yyy format will be merged into\n",
      "                        config file (deprecate), change to --cfg-options\n",
      "                        instead.\n",
      "  --cfg-options CFG_OPTIONS [CFG_OPTIONS ...]\n",
      "                        override some settings in the used config, the key-\n",
      "                        value pair in xxx=yyy format will be merged into\n",
      "                        config file. If the value to be overwritten is a list,\n",
      "                        it should be like key=\"[a,b]\" or key=a,b It also\n",
      "                        allows nested list/tuple values, e.g.\n",
      "                        key=\"[(a,b),(c,d)]\" Note that the quotation marks are\n",
      "                        necessary and that no white space is allowed.\n",
      "  --launcher {none,pytorch,slurm,mpi}\n",
      "                        job launcher\n",
      "  --local_rank LOCAL_RANK\n",
      "  --auto-scale-lr       enable automatically scaling LR.\n"
     ]
    }
   ],
   "source": [
    "# 可以先看看用法指南，不過蠻多的也容易混亂，可以參考後面簡單的使用方式\n",
    "!python tools/train.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最簡單來個config檔就結束了\n",
    "!python tools/train.py pisa_chess.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b324149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若想試試看不加PISA的結果也可以download相對版本:\n",
    "\n",
    "# !mim download mmdet --config faster_rcnn_x101_32x4d_fpn_1x_coco --dest ./downloads\n",
    "# !python tools/train.py fasterrcnn_chess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5371724",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c544cf77",
   "metadata": {},
   "source": [
    "請練習自己基於pisa_chess.py改寫一個configure file, 將:\n",
    "1. nms_pre數量調整為2000\n",
    "2. 總 epoch數調整為20\n",
    "3. optimizer的learning rate初始值調為0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39848dd4",
   "metadata": {},
   "source": [
    "hint:\n",
    "新增一個script: e.g. answer_2.py，內容包含\n",
    "```python\n",
    "    _base_ = [??]\n",
    "    model = dict(??)\n",
    "    optimizer = ?? 0.001 ??\n",
    "    ?? = ?? max_epochs=20 ??\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fc8ae5",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb92ebff",
   "metadata": {},
   "source": [
    "* [PISA] Cao, Y., Chen, K., Loy, C. C., & Lin, D. (2020). Prime sample attention in object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 11583-11591).\n",
    "* [ResNext] Xie, S., Girshick, R., Dollár, P., Tu, Z., & He, K. (2017). Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1492-1500).\n",
    "* [FPN] Lin, T. Y., Dollár, P., Girshick, R., He, K., Hariharan, B., & Belongie, S. (2017). Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2117-2125).\n",
    "* [COCO Dataset] Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., ... & Zitnick, C. L. (2014, September). Microsoft coco: Common objects in context. In European conference on computer vision (pp. 740-755). Springer, Cham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e63ff6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
