{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba833185",
   "metadata": {},
   "source": [
    "# Training PISA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58098798",
   "metadata": {},
   "source": [
    "$^*$: 意指詳見reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ecc18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前面要先灌mmdetection\n",
    "!pip install openmim\n",
    "!mim install mmdet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615584d6",
   "metadata": {},
   "source": [
    "## 1.Configure file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c22c5",
   "metadata": {},
   "source": [
    "### 1.1.命名風格\n",
    "MMDetection對於Configure File的命名風格如下：\n",
    "{model}\\_{backbone}\\_{neck}\\_{schedule}\\_{dataset}\n",
    "\n",
    "- {model}： model 種類，例如：faster_rcnn, mask_rcnn 等。\n",
    "- {backbone}：Backbone 種類，例如：r50 (ResNet-50), x101 (ResNeXt-101) 等。\n",
    "- {neck}：Neck 種類，例如：fpn, pafpn, nasfpn, c4 等。\n",
    "- {schedule}： 訓練方案，選項是 1x、 2x、 20e 等。\n",
    "> 1x 和 2x 分別代表 12 epoch 和 24 epoch，20e 在級聯 (Cascaded) 模型中使用，表示 20 epoch。\n",
    "- {dataset}：資料集，例如：coco、 cityscapes、 voc_0712、 wider_face 等。\n",
    "\n",
    "例如pisa_faster_rcnn_x101_32x4d_fpn_1x_coco.py的意思就是:\n",
    "- 使用Prime sample attention(PISA)$^*$方案的FasterRCNN$^*$物件偵測模型 \n",
    "- Backbone用的是ResNext101$^*$，每個cell用的是32個group做concatenate，並且寬度為4\n",
    "- Neck用的是Feature Pyramid Networks(FPN)$^*$，在YOLOv4也有用\n",
    "- 訓練上這邊用的是 12 epoch\n",
    "- 訓練在coco dataset$^*$上"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e5a91a",
   "metadata": {},
   "source": [
    "### 1.2基本內容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9912b",
   "metadata": {},
   "source": [
    "作者為了可以提高代碼複用率，所以 config 支持繼承的操作，通過 \\_base\\_ 變量來實現，\\_base\\_ 是一個 list 類型變量，裡面存放的是要繼承配置文件的路徑，任何配置文件都能往上追朔繼承以下四種類型的文件：\n",
    "- 模型 (models)\n",
    "- 資料集 (datasets)\n",
    "- 訓練策略 (schedules)\n",
    "- 運行時的默認配置 (default_runtime)\n",
    "\n",
    "在運算過程中mmdetection會把多個config檔整合成一個，所以後面我們下載下來的基本pisa config檔只有一個\n",
    "\n",
    "這邊看一下pisa_faster_rcnn_x101_32x4d_fpn_1x_coco.py 長怎樣："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a93c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝mmdet時會順便安裝mmcv，是mm系列有關cv演算法的核心，其中包含config讀取\n",
    "from mmcv import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebdc4276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing pisa_faster_rcnn_x101_32x4d_fpn_1x_coco...\n",
      "\u001b[?25l  [####################################]  100%          \u001b[?25h\n",
      "\u001b[32mSuccessfully downloaded pisa_faster_rcnn_x101_32x4d_fpn_1x_coco-e4accec4.pth to /home/jovyan/course/pisa-simple-example\u001b[0m\n",
      "\u001b[32mSuccessfully dumped pisa_faster_rcnn_x101_32x4d_fpn_1x_coco.py to /home/jovyan/course/pisa-simple-example\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 沒有這份config檔請下載\n",
    "!mim download mmdet --config pisa_faster_rcnn_x101_32x4d_fpn_1x_coco --dest .\n",
    "cfg=Config.fromfile(\"pisa_faster_rcnn_x101_32x4d_fpn_1x_coco.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb32e6ec",
   "metadata": {},
   "source": [
    "大致上包含資料、模型、訓練、測試、推論的每個設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "299ff138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'dataset_type', 'data_root', 'img_norm_cfg', 'train_pipeline', 'test_pipeline', 'data', 'evaluation', 'optimizer', 'optimizer_config', 'lr_config', 'runner', 'checkpoint_config', 'log_config', 'custom_hooks', 'dist_params', 'log_level', 'load_from', 'resume_from', 'workflow', 'opencv_num_threads', 'mp_start_method', 'auto_scale_lr'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bdde49",
   "metadata": {},
   "source": [
    "#### 1.2.1.Model設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b92f8a",
   "metadata": {},
   "source": [
    "Model裡面就把架構有系統的建構方式設定起來，也有有關訓練和測試的設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bfd5ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['type', 'backbone', 'neck', 'rpn_head', 'roi_head', 'train_cfg', 'test_cfg'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0727b2",
   "metadata": {},
   "source": [
    "##### **一些非重點的設定**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f49eb67",
   "metadata": {},
   "source": [
    "**類型**\n",
    "\n",
    "物件偵測模型類型，由於PISA演算法的核心是在roi_head的cnn blocks中實施self attention，所以與這個無關\n",
    "\n",
    "但是在論文上主要有使用到FasterRCNN來做主架構更動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acf966dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FasterRCNN'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b25f62a",
   "metadata": {},
   "source": [
    "**Backbone模型**\n",
    "\n",
    "同個物件偵測模型中的backbone模型可以換置，會影響圖片的embedding效果，我們這邊用論文中做得較好的ResNext模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c52eab56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'ResNeXt',\n",
       " 'depth': 101,\n",
       " 'num_stages': 4,\n",
       " 'out_indices': (0, 1, 2, 3),\n",
       " 'frozen_stages': 1,\n",
       " 'norm_cfg': {'type': 'BN', 'requires_grad': True},\n",
       " 'norm_eval': True,\n",
       " 'style': 'pytorch',\n",
       " 'init_cfg': {'type': 'Pretrained',\n",
       "  'checkpoint': 'open-mmlab://resnext101_32x4d'},\n",
       " 'groups': 32,\n",
       " 'base_width': 4}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a58d8",
   "metadata": {},
   "source": [
    "**Neck模型**\n",
    "\n",
    "Neck 模型來將對應不同scale投射到不同個輸出時使用，使用FPN(*ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82dd1d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'FPN',\n",
       " 'in_channels': [256, 512, 1024, 2048],\n",
       " 'out_channels': 256,\n",
       " 'num_outs': 5}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.neck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fc9d8b",
   "metadata": {},
   "source": [
    "**Region Proposal Network Head**\n",
    "\n",
    "使用Region Proposal Network(RPN，詳見FasterRCNN論文) 以前面Neck的資訊做物件區域偵測的提案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a6a6f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'RPNHead',\n",
       " 'in_channels': 256,\n",
       " 'feat_channels': 256,\n",
       " 'anchor_generator': {'type': 'AnchorGenerator',\n",
       "  'scales': [8],\n",
       "  'ratios': [0.5, 1.0, 2.0],\n",
       "  'strides': [4, 8, 16, 32, 64]},\n",
       " 'bbox_coder': {'type': 'DeltaXYWHBBoxCoder',\n",
       "  'target_means': [0.0, 0.0, 0.0, 0.0],\n",
       "  'target_stds': [1.0, 1.0, 1.0, 1.0]},\n",
       " 'loss_cls': {'type': 'CrossEntropyLoss',\n",
       "  'use_sigmoid': True,\n",
       "  'loss_weight': 1.0},\n",
       " 'loss_bbox': {'type': 'L1Loss', 'loss_weight': 1.0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.rpn_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2023d70",
   "metadata": {},
   "source": [
    "##### **Region of Interest Head**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60740229",
   "metadata": {},
   "source": [
    "使用Region of Interest(ROI，詳見FasterRCNN論文)head 將最終預測算出。\n",
    "\n",
    "*這邊就是PISA將Attention加進model的地方*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88700823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'PISARoIHead',\n",
       " 'bbox_roi_extractor': {'type': 'SingleRoIExtractor',\n",
       "  'roi_layer': {'type': 'RoIAlign', 'output_size': 7, 'sampling_ratio': 0},\n",
       "  'out_channels': 256,\n",
       "  'featmap_strides': [4, 8, 16, 32]},\n",
       " 'bbox_head': {'type': 'Shared2FCBBoxHead',\n",
       "  'in_channels': 256,\n",
       "  'fc_out_channels': 1024,\n",
       "  'roi_feat_size': 7,\n",
       "  'num_classes': 80,\n",
       "  'bbox_coder': {'type': 'DeltaXYWHBBoxCoder',\n",
       "   'target_means': [0.0, 0.0, 0.0, 0.0],\n",
       "   'target_stds': [0.1, 0.1, 0.2, 0.2]},\n",
       "  'reg_class_agnostic': False,\n",
       "  'loss_cls': {'type': 'CrossEntropyLoss',\n",
       "   'use_sigmoid': False,\n",
       "   'loss_weight': 1.0},\n",
       "  'loss_bbox': {'type': 'SmoothL1Loss', 'loss_weight': 1.0, 'beta': 1.0}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.roi_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797c341",
   "metadata": {},
   "source": [
    "#### 1.2.2.資料設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab5836",
   "metadata": {},
   "source": [
    "- xxx_per_gpu: 一些GPU設定\n",
    "- data.train/val/test: 訓練、驗證、測試集都有不同的設定 (下面以train來講解)\n",
    "- data.train.type: 資料集類型，主要是標註的格式\n",
    "- data.train.ann_file: 標註檔位置\n",
    "- data.train.img_prefix: 圖檔資料夾位置\n",
    "- data.train.pipeline: 用list方式列出pre-process會用的演算法，可能有normalize大小、翻轉、旋轉等等可以用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49dc9864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'samples_per_gpu': 2,\n",
       " 'workers_per_gpu': 2,\n",
       " 'train': {'type': 'CocoDataset',\n",
       "  'ann_file': 'data/coco/annotations/instances_train2017.json',\n",
       "  'img_prefix': 'data/coco/train2017/',\n",
       "  'pipeline': [{'type': 'LoadImageFromFile'},\n",
       "   {'type': 'LoadAnnotations', 'with_bbox': True},\n",
       "   {'type': 'Resize', 'img_scale': (1333, 800), 'keep_ratio': True},\n",
       "   {'type': 'RandomFlip', 'flip_ratio': 0.5},\n",
       "   {'type': 'Normalize',\n",
       "    'mean': [123.675, 116.28, 103.53],\n",
       "    'std': [58.395, 57.12, 57.375],\n",
       "    'to_rgb': True},\n",
       "   {'type': 'Pad', 'size_divisor': 32},\n",
       "   {'type': 'DefaultFormatBundle'},\n",
       "   {'type': 'Collect', 'keys': ['img', 'gt_bboxes', 'gt_labels']}]},\n",
       " 'val': {'type': 'CocoDataset',\n",
       "  'ann_file': 'data/coco/annotations/instances_val2017.json',\n",
       "  'img_prefix': 'data/coco/val2017/',\n",
       "  'pipeline': [{'type': 'LoadImageFromFile'},\n",
       "   {'type': 'MultiScaleFlipAug',\n",
       "    'img_scale': (1333, 800),\n",
       "    'flip': False,\n",
       "    'transforms': [{'type': 'Resize', 'keep_ratio': True},\n",
       "     {'type': 'RandomFlip'},\n",
       "     {'type': 'Normalize',\n",
       "      'mean': [123.675, 116.28, 103.53],\n",
       "      'std': [58.395, 57.12, 57.375],\n",
       "      'to_rgb': True},\n",
       "     {'type': 'Pad', 'size_divisor': 32},\n",
       "     {'type': 'ImageToTensor', 'keys': ['img']},\n",
       "     {'type': 'Collect', 'keys': ['img']}]}]},\n",
       " 'test': {'type': 'CocoDataset',\n",
       "  'ann_file': 'data/coco/annotations/instances_val2017.json',\n",
       "  'img_prefix': 'data/coco/val2017/',\n",
       "  'pipeline': [{'type': 'LoadImageFromFile'},\n",
       "   {'type': 'MultiScaleFlipAug',\n",
       "    'img_scale': (1333, 800),\n",
       "    'flip': False,\n",
       "    'transforms': [{'type': 'Resize', 'keep_ratio': True},\n",
       "     {'type': 'RandomFlip'},\n",
       "     {'type': 'Normalize',\n",
       "      'mean': [123.675, 116.28, 103.53],\n",
       "      'std': [58.395, 57.12, 57.375],\n",
       "      'to_rgb': True},\n",
       "     {'type': 'Pad', 'size_divisor': 32},\n",
       "     {'type': 'ImageToTensor', 'keys': ['img']},\n",
       "     {'type': 'Collect', 'keys': ['img']}]}]}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f69df73",
   "metadata": {},
   "source": [
    "沒資料的話請下載資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1521dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/coco\n",
    "!curl -L \"https://public.roboflow.com/ds/teSUuBdtOy?key=MSfxYe5Fz3\" > data/coco/roboflow.zip\n",
    "!unzip -f data/coco/roboflow.zip -d data/coco\n",
    "!rm -rf data/coco/roboflow.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11244e03",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] [--work-dir WORK_DIR] [--resume-from RESUME_FROM]\n",
      "                [--auto-resume] [--no-validate]\n",
      "                [--gpus GPUS | --gpu-ids GPU_IDS [GPU_IDS ...] | --gpu-id\n",
      "                GPU_ID] [--seed SEED] [--diff-seed] [--deterministic]\n",
      "                [--options OPTIONS [OPTIONS ...]]\n",
      "                [--cfg-options CFG_OPTIONS [CFG_OPTIONS ...]]\n",
      "                [--launcher {none,pytorch,slurm,mpi}]\n",
      "                [--local_rank LOCAL_RANK] [--auto-scale-lr]\n",
      "                config\n",
      "\n",
      "Train a detector\n",
      "\n",
      "positional arguments:\n",
      "  config                train config file path\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --work-dir WORK_DIR   the dir to save logs and models\n",
      "  --resume-from RESUME_FROM\n",
      "                        the checkpoint file to resume from\n",
      "  --auto-resume         resume from the latest checkpoint automatically\n",
      "  --no-validate         whether not to evaluate the checkpoint during training\n",
      "  --gpus GPUS           (Deprecated, please use --gpu-id) number of gpus to\n",
      "                        use (only applicable to non-distributed training)\n",
      "  --gpu-ids GPU_IDS [GPU_IDS ...]\n",
      "                        (Deprecated, please use --gpu-id) ids of gpus to use\n",
      "                        (only applicable to non-distributed training)\n",
      "  --gpu-id GPU_ID       id of gpu to use (only applicable to non-distributed\n",
      "                        training)\n",
      "  --seed SEED           random seed\n",
      "  --diff-seed           Whether or not set different seeds for different ranks\n",
      "  --deterministic       whether to set deterministic options for CUDNN\n",
      "                        backend.\n",
      "  --options OPTIONS [OPTIONS ...]\n",
      "                        override some settings in the used config, the key-\n",
      "                        value pair in xxx=yyy format will be merged into\n",
      "                        config file (deprecate), change to --cfg-options\n",
      "                        instead.\n",
      "  --cfg-options CFG_OPTIONS [CFG_OPTIONS ...]\n",
      "                        override some settings in the used config, the key-\n",
      "                        value pair in xxx=yyy format will be merged into\n",
      "                        config file. If the value to be overwritten is a list,\n",
      "                        it should be like key=\"[a,b]\" or key=a,b It also\n",
      "                        allows nested list/tuple values, e.g.\n",
      "                        key=\"[(a,b),(c,d)]\" Note that the quotation marks are\n",
      "                        necessary and that no white space is allowed.\n",
      "  --launcher {none,pytorch,slurm,mpi}\n",
      "                        job launcher\n",
      "  --local_rank LOCAL_RANK\n",
      "  --auto-scale-lr       enable automatically scaling LR.\n"
     ]
    }
   ],
   "source": [
    "# 可以先看看用法指南，不過蠻多的也容易混亂，可以參考後面簡單的使用方式\n",
    "!python tools/train.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最簡單來個config檔就結束了\n",
    "!python tools/train.py train_config.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2ca44b",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759a1d5",
   "metadata": {},
   "source": [
    "* [PISA] Cao, Y., Chen, K., Loy, C. C., & Lin, D. (2020). Prime sample attention in object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 11583-11591).\n",
    "* [ResNext] Xie, S., Girshick, R., Dollár, P., Tu, Z., & He, K. (2017). Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1492-1500).\n",
    "* [FPN] Lin, T. Y., Dollár, P., Girshick, R., He, K., Hariharan, B., & Belongie, S. (2017). Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2117-2125).\n",
    "* [COCO Dataset] Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., ... & Zitnick, C. L. (2014, September). Microsoft coco: Common objects in context. In European conference on computer vision (pp. 740-755). Springer, Cham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633434ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
